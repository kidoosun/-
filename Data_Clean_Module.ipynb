{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "637c9523-bd60-45af-a17e-605782a8dd59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Clean Module for Title & Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e0a9f-da18-442f-8aa2-53d635653ea5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9360c69f-771d-454b-a89d-90dd8c2c2416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import unidecode \n",
    "import pandas as pd \n",
    "import re \n",
    "import time \n",
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "nltk.download('stopwords') \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from autocorrect import Speller \n",
    "from bs4 import BeautifulSoup \n",
    "from nltk.corpus import stopwords \n",
    "from nltk import word_tokenize \n",
    "import string "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5eb1f-4dcb-42c4-91ea-bdd5f6ea575b",
   "metadata": {},
   "source": [
    "## Clean Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68d3d7ae-9713-49e6-ae20-4f6b3a640e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_newlines_tabs(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of newlines, tabs, and combinations like: \\\\n, \\\\.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of newlines, tabs, \\\\n, \\\\ characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is her \\\\ first day at this place.\\n Please,\\t Be nice to her.\\\\n\n",
    "    Output : This is her first day at this place. Please, Be nice to her. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Replacing all the occurrences of \\n,\\\\n,\\t,\\\\ with a space.\n",
    "    Formatted_text = text.replace('\\\\n', ' ').replace('\\n', ' ').replace('\\t',' ').replace('\\\\', ' ').replace('. com', '.com')\n",
    "    return Formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfddea2-fa8f-4296-ab7d-43d2b8418794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_tags(text):\n",
    "    \"\"\" \n",
    "    This function will remove all the occurrences of html tags from the text.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of html tags.\n",
    "        \n",
    "    Example:\n",
    "    Input : This is a nice place to live. <IMG>\n",
    "    Output : This is a nice place to live.  \n",
    "    \"\"\"\n",
    "    # Initiating BeautifulSoup object soup.\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get all the text other than html tags.\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf86f59-6c6d-49c6-99ed-ab7ba137ee15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(text):\n",
    "    \"\"\"\n",
    "    This function will remove all the occurrences of links.\n",
    "    \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after removal of all types of links.\n",
    "        \n",
    "    Example:\n",
    "    Input : To know more about this website: kajalyadav.com  visit: https://kajalyadav.com//Blogs\n",
    "    Output : To know more about this website: visit:     \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Removing all the occurrences of links that starts with https\n",
    "    remove_https = re.sub(r'http\\S+', '', text)\n",
    "    # Remove all the occurrences of text that ends with .com\n",
    "    remove_com = re.sub(r\"\\ [A-Za-z]*\\.com\", \" \", remove_https)\n",
    "    # Remove all .com\n",
    "    remove_com1 = re.sub(r\"[A-Za-z]*.com\", \" \", remove_com)\n",
    "    return remove_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98cd89d-8a6e-4373-84e3-3982b45eba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    \"\"\" This function will remove \n",
    "        extra whitespaces from the text\n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" after extra whitespaces removed .\n",
    "        \n",
    "    Example:\n",
    "    Input : How   are   you   doing   ?\n",
    "    Output : How are you doing ?     \n",
    "        \n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'\\s+') \n",
    "    Without_whitespace = re.sub(pattern, ' ', text)\n",
    "    # There are some instances where there is no space after '?' & ')', \n",
    "    # So I am replacing these with one space so that It will not consider two words as one token.\n",
    "    text = Without_whitespace.replace('?', ' ? ').replace(')', ') ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94091eac-507b-4d99-8b72-a2378a2c4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for accented characters removal\n",
    "def accented_characters_removal(text):\n",
    "    # this is a docstring\n",
    "    \"\"\"\n",
    "    The function will remove accented characters from the \n",
    "    text contained within the Dataset.\n",
    "       \n",
    "    arguments:\n",
    "        input_text: \"text\" of type \"String\". \n",
    "                    \n",
    "    return:\n",
    "        value: \"text\" with removed accented characters.\n",
    "        \n",
    "    Example:\n",
    "    Input : Málaga, àéêöhello\n",
    "    Output : Malaga, aeeohello    \n",
    "        \n",
    "    \"\"\"\n",
    "    # Remove accented characters from text using unidecode.\n",
    "    # Unidecode() - It takes unicode data & tries to represent it to ASCII characters. \n",
    "    text = unidecode.unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c301723c-e708-4183-8780-71e1cac89a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for text lowercasing\n",
    "def lower_casing_text(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    The function will convert text into lower case.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "         value: text in lowercase\n",
    "         \n",
    "    Example:\n",
    "    Input : The World is Full of Surprises!\n",
    "    Output : the world is full of surprises!\n",
    "    \n",
    "    \"\"\"\n",
    "    # Convert text to lower case\n",
    "    # lower() - It converts all upperase letter of given string to lowercase.\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bed1961-7007-41f5-87d2-1b2857d4072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for removing repeated characters and punctuations\n",
    "\n",
    "def reducing_incorrect_character_repeatation(text):\n",
    "    \"\"\"\n",
    "    This Function will reduce repeatition to two characters \n",
    "    for alphabets and to one character for punctuations.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Finally formatted text with alphabets repeating to \n",
    "        two characters & punctuations limited to one repeatition \n",
    "        \n",
    "    Example:\n",
    "    Input : Realllllllllyyyyy,        Greeeeaaaatttt   !!!!?....;;;;:)\n",
    "    Output : Reallyy, Greeaatt !?.;:)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Pattern matching for all case alphabets\n",
    "    Pattern_alpha = re.compile(r\"([A-Za-z])\\1{1,}\", re.DOTALL)\n",
    "    \n",
    "    # Limiting all the  repeatation to two characters.\n",
    "    Formatted_text = Pattern_alpha.sub(r\"\\1\\1\", text) \n",
    "    \n",
    "    # Pattern matching for all the punctuations that can occur\n",
    "    Pattern_Punct = re.compile(r'([.,/#!$%^&*?;:{}=_`~()+-])\\1{1,}')\n",
    "    \n",
    "    # Limiting punctuations in previously formatted string to only one.\n",
    "    Combined_Formatted = Pattern_Punct.sub(r'\\1', Formatted_text)\n",
    "    \n",
    "    # The below statement is replacing repeatation of spaces that occur more than two times with that of one occurrence.\n",
    "    Final_Formatted = re.sub(' {2,}',' ', Combined_Formatted)\n",
    "    return Final_Formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266906f6-2c19-4490-9403-555fba41da42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "}\n",
    "# The code for expanding contraction words\n",
    "def expand_contractions(text, contraction_mapping =  CONTRACTION_MAP):\n",
    "    \"\"\"expand shortened words to the actual form.\n",
    "       e.g. don't to do not\n",
    "    \n",
    "       arguments:\n",
    "            input_text: \"text\" of type \"String\".\n",
    "         \n",
    "       return:\n",
    "            value: Text with expanded form of shorthened words.\n",
    "        \n",
    "       Example: \n",
    "       Input : ain't, aren't, can't, cause, can't've\n",
    "       Output :  is not, are not, cannot, because, cannot have \n",
    "    \n",
    "     \"\"\"\n",
    "    # Tokenizing text into tokens.\n",
    "    list_Of_tokens = text.split(' ')\n",
    "\n",
    "    # Checking for whether the given token matches with the Key & replacing word with key's value.\n",
    "    \n",
    "    # Check whether Word is in lidt_Of_tokens or not.\n",
    "    for Word in list_Of_tokens: \n",
    "        # Check whether found word is in dictionary \"Contraction Map\" or not as a key. \n",
    "         if Word in CONTRACTION_MAP: \n",
    "                # If Word is present in both dictionary & list_Of_tokens, replace that word with the key value.\n",
    "                list_Of_tokens = [item.replace(Word, CONTRACTION_MAP[Word]) for item in list_Of_tokens]\n",
    "                \n",
    "    # Converting list of tokens to String.\n",
    "    String_Of_tokens = ' '.join(str(e) for e in list_Of_tokens) \n",
    "    return String_Of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5abacd67-5489-4dfd-869b-b1b62e364bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for removing special characters\n",
    "def removing_special_characters(text):\n",
    "    \"\"\"Removing all the special characters except the one that is passed within \n",
    "       the regex to match, as they have imp meaning in the text provided.\n",
    "   \n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text with removed special characters that don't require.\n",
    "        \n",
    "    Example: \n",
    "    Input : Hello, K-a-j-a-l. Thi*s is $100.05 : the payment that you will recieve! (Is this okay?) \n",
    "    Output :  Hello, Kajal. This is $100.05 : the payment that you will recieve! Is this okay?\n",
    "    \n",
    "   \"\"\"\n",
    "    # The formatted text after removing not necessary punctuations.\n",
    "    # Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text) \n",
    "    # In the above regex expression,I am providing necessary set of punctuations that are frequent in this particular dataset.\n",
    "    Formatted_Text = re.sub(r\"[^a-zA-Z0-9:$-,%.?!]+\", ' ', text)\n",
    "    return Formatted_Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "782a06a6-0e0c-411e-a164-7bd35b5e269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for removing stopwords\n",
    "stoplist = stopwords.words('english') \n",
    "stoplist = set(stoplist)\n",
    "def removing_stopwords(text):\n",
    "    \"\"\"This function will remove stopwords which doesn't add much meaning to a sentence \n",
    "       & they can be remove safely without comprimising meaning of the sentence.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after omitted all stopwords.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Kajal from delhi who came here to study.\n",
    "    Output : [\"'This\", 'Kajal', 'delhi', 'came', 'study', '.', \"'\"] \n",
    "    \n",
    "   \"\"\"\n",
    "    # repr() function actually gives the precise information about the string\n",
    "    text = repr(text)\n",
    "    # Text without stopwords\n",
    "    No_StopWords = [word for word in word_tokenize(text) if word.lower() not in stoplist ]\n",
    "    # Convert list of tokens_without_stopwords to String type.\n",
    "    words_string = ' '.join(No_StopWords)    \n",
    "    return words_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38034f52-f054-49e8-8455-d211e8a9fb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for spelling corrections\n",
    "def spelling_correction(text):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a391a0e6-8150-4aab-acb5-4b618452bd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for spelling corrections\n",
    "def spelling_correction(text):\n",
    "    ''' \n",
    "    This function will correct spellings.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text after corrected spellings.\n",
    "        \n",
    "    Example: \n",
    "    Input : This is Oberois from Dlhi who came heree to studdy.\n",
    "    Output : This is Oberoi from Delhi who came here to study.\n",
    "      \n",
    "    \n",
    "    '''\n",
    "    # Check for spellings in English language\n",
    "    spell = Speller(lang='en')\n",
    "    Corrected_text = spell(text)\n",
    "    return Corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3db584ce-02af-4c93-9d26-4b4597b49598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for lemmatization\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatization(text):\n",
    "    \"\"\"This function converts word to their root words \n",
    "       without explicitely cut down as done in stemming.\n",
    "    \n",
    "    arguments:\n",
    "         input_text: \"text\" of type \"String\".\n",
    "         \n",
    "    return:\n",
    "        value: Text having root words only, no tense form, no plural forms\n",
    "        \n",
    "    Example: \n",
    "    Input : text reduced \n",
    "    Output :  text reduce\n",
    "    \n",
    "   \"\"\"\n",
    "    # Converting words to their root forms\n",
    "    lemma = [lemmatizer.lemmatize(w,'v') for w in w_tokenizer.tokenize(text)]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f15ebd7-8eb6-41d4-92fc-a397859556a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing main function to merge all the preprocessing steps.\n",
    "def text_preprocessing(text, accented_chars=True, contractions=False, lemmatization = False,\n",
    "                        extra_whitespace=True, newlines_tabs=True, repeatition=True, \n",
    "                       lowercase=False, punctuations=True, mis_spell=False,\n",
    "                       remove_html=True, links=True,  special_chars=True,\n",
    "                       stop_words=False):\n",
    "    \"\"\"\n",
    "    This function will preprocess input text and return\n",
    "    the clean text.\n",
    "    \"\"\"\n",
    "        \n",
    "    if newlines_tabs == True: #remove newlines & tabs.\n",
    "        Data = remove_newlines_tabs(text)\n",
    "        \n",
    "    if remove_html == True: #remove html tags\n",
    "        Data = strip_html_tags(Data)\n",
    "        \n",
    "    if links == True: #remove links\n",
    "        Data = remove_links(Data)\n",
    "        \n",
    "    if extra_whitespace == True: #remove extra whitespaces\n",
    "        Data = remove_whitespace(Data)\n",
    "        \n",
    "    if accented_chars == True: #remove accented characters\n",
    "        Data = accented_characters_removal(Data)\n",
    "        \n",
    "    if lowercase == True: #convert all characters to lowercase\n",
    "        Data = lower_casing_text(Data)\n",
    "        \n",
    "    if repeatition == True: #Reduce repeatitions   \n",
    "        Data = reducing_incorrect_character_repeatation(Data)\n",
    "        \n",
    "    if contractions == True: #expand contractions\n",
    "        Data = expand_contractions(Data)\n",
    "    \n",
    "    if punctuations == True: #remove punctuations\n",
    "        Data = removing_special_characters(Data)\n",
    "    \n",
    "    stoplist = stopwords.words('english') \n",
    "    stoplist = set(stoplist)\n",
    "    \n",
    "    if stop_words == True: #Remove stopwords\n",
    "        Data = removing_stopwords(Data)\n",
    "        \n",
    "    spell = Speller(lang='en')\n",
    "    \n",
    "    if mis_spell == True: #Check for mis-spelled words & correct them.\n",
    "        Data = spelling_correction(Data)\n",
    "        \n",
    "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "     \n",
    "    if lemmatization == True: #Converts words to lemma form.\n",
    "        Data = lemmatization(Data)\n",
    "    \n",
    "           \n",
    "    return Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00878112-b173-4fdd-9695-76200d00776f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read and Clean Content Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5f929aa-1ff2-4fbd-be83-ab570cb3eb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16317, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Content Dataset \n",
    "df = pd.read_csv(\"D://NLP//Frame_NLP//archive//coronavirus_content.txt\", index_col = False, sep='delimiter', on_bad_lines='skip', header=None, encoding = 'utf8')\n",
    "#DF = pd.read_csv(\"D://NLP//Frame_NLP//archive//covid19_content.txt\", index_col = False, sep='delimiter', encoding = 'utf-8')\n",
    "# Show Dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e966adb-0b07-4c41-9362-b92e065bfdf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Photo illustration by Slate. Photo by Photo il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the U.S., COVID-19 is spreading like wildfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Researchers and medical practitioners have spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It won’t be. The updated message that needs to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A big part of the challenges around messaging ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16312</th>\n",
       "      <td>CHISINAU (UrduPoint News / Sputnik - 23rd Marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16313</th>\n",
       "      <td>The first two deaths caused by the coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16314</th>\n",
       "      <td>\"The third Romanian died on the evening of Sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16315</th>\n",
       "      <td>The latest update from the Romanian government...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16316</th>\n",
       "      <td>Your Thoughts and Comments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16317 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0      Photo illustration by Slate. Photo by Photo il...\n",
       "1      In the U.S., COVID-19 is spreading like wildfi...\n",
       "2      Researchers and medical practitioners have spe...\n",
       "3      It won’t be. The updated message that needs to...\n",
       "4      A big part of the challenges around messaging ...\n",
       "...                                                  ...\n",
       "16312  CHISINAU (UrduPoint News / Sputnik - 23rd Marc...\n",
       "16313  The first two deaths caused by the coronavirus...\n",
       "16314  \"The third Romanian died on the evening of Sun...\n",
       "16315  The latest update from the Romanian government...\n",
       "16316                         Your Thoughts and Comments\n",
       "\n",
       "[16317 rows x 1 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df.dropna(axis=0, how='any', inplace=False)\n",
    "df_all\n",
    "df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d31bce6-5dab-4d9f-8bdf-6dd09050f609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1089081...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1116638...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1116497...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1115214...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://twitter.com/tatianacirisano/status/1235271964656308226\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://twitter.com/QuitaC_KVUE/status/1236031948423864324\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=OOJqHPfG7pA=emb_logo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=oGruT7Fd54E=emb_title\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/tech/1124521...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1126061...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1126885...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1126643...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1126345...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\bs4\\__init__.py:431: MarkupResemblesLocatorWarning: \"https://www.thesun.co.uk/news/1125912...\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing for Content\n",
    "List_Content = df_all[0].to_list()\n",
    "Final_Article = []\n",
    "Complete_Content = []\n",
    "for article in List_Content:\n",
    "    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n",
    "    Final_Article.append(Processed_Content)\n",
    "Complete_Content.extend(Final_Article)\n",
    "df_all['Processed_Content'] = Complete_Content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e59d1092-2de8-4cc3-87d7-05d125fbc696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The coronavirus is airborne that statement is jarring. It conveys that something harmful can be present, even when it cannot be seen with the naked eye, or felt on the skin. Many people have already heard the expression it's airborne in the context of Outbreak, the 1995 Dustin Hoffman thriller (and the fifth most popular movie on Netflix in March!) . Its already associate it with a life threatening disease. A concise warning lends itself to repetition, a key tactic in getting an idea across. Most importantly, the coronavirus is airborne provides direct support for precautionary measures for preventing the spread of COVID 19, such as keeping at least six feet of distance from people who are not in your household, wearing a face covering over your nose and mouth when in public, spending the bare minimum amount of time in indoor spaces that aren't your home, and improving the ventilation in buildings. (Surface transmission might be less common, but, yes, it's still important to wash your hands with soap and water.) If you're going to be inside for a long time with people from a variety of households say, at a school care should be taken to ensure that the chance someone with an infection is there is very low.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Complete_Content[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37556fab-87d2-44ec-b16f-f04702814207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Processed_Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Photo illustration by Slate. Photo by Photo il...</td>\n",
       "      <td>Photo illustration by Slate. Photo by Photo il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the U.S., COVID-19 is spreading like wildfi...</td>\n",
       "      <td>In the U.S., COVID 19 is spreading like wildfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Researchers and medical practitioners have spe...</td>\n",
       "      <td>Researchers and medical practitioners have spe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It won’t be. The updated message that needs to...</td>\n",
       "      <td>It won't be. The updated message that needs to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A big part of the challenges around messaging ...</td>\n",
       "      <td>A big part of the challenges around messaging ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  Photo illustration by Slate. Photo by Photo il...   \n",
       "1  In the U.S., COVID-19 is spreading like wildfi...   \n",
       "2  Researchers and medical practitioners have spe...   \n",
       "3  It won’t be. The updated message that needs to...   \n",
       "4  A big part of the challenges around messaging ...   \n",
       "\n",
       "                                   Processed_Content  \n",
       "0  Photo illustration by Slate. Photo by Photo il...  \n",
       "1  In the U.S., COVID 19 is spreading like wildfi...  \n",
       "2  Researchers and medical practitioners have spe...  \n",
       "3  It won't be. The updated message that needs to...  \n",
       "4  A big part of the challenges around messaging ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4ee63b4-7e58-4cba-a0a3-95ec340680f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.drop([0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea38fd43-383f-4f43-9b44-61e013181892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16317, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d352af-e7e3-4fb5-a08f-f7f716af219c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12224, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.drop_duplicates(keep = False, inplace = True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc798dd-d2e8-483b-8e67-fd296ba19810",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spec_chars = [\"#\",\"@\",\"…\",\"\\xa0\",\"\\n\\n\",\"\\n\",\"\\\\\"]\n",
    "for char in spec_chars:\n",
    "    df_all = df_all.str.replace(char, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b081f7ff-aa35-4f00-9540-cc317c6509a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('D://NLP//Frame_NLP//archive//coronavirus_content.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292ae53-a005-4bcb-bcc4-f611a44115fc",
   "metadata": {},
   "source": [
    "## Content Data to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a3a7b65-49e2-451a-a67c-9fbe6bc4bb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "\n",
    "spacy.require_gpu()\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2500000\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "phrase_matcher = PhraseMatcher(nlp.vocab)\n",
    "phrases = ['covid19','COVID-19','Covid19','COVID19','coronavirus','covid']\n",
    "patterns = [nlp(text) for text in phrases]\n",
    "phrase_matcher.add('covid19', None, *patterns)\n",
    "with open(\"D://NLP//Frame_NLP//archive//coronavirus_content.csv\", encoding='utf8') as f:\n",
    "    text = f.read().replace(\"\\n\\n\", \" \").replace(\"\\n\", \" \")\n",
    "doc = nlp(text)\n",
    "\n",
    "with open(\"D://NLP//Frame_NLP//archive//coronavirus_content_sent.txt\", 'w', encoding='utf8') as f:\n",
    "    for sent in doc.sents:\n",
    "        for match_id, start, end in phrase_matcher(nlp(sent.text)):\n",
    "            if nlp.vocab.strings[match_id] in [\"covid19\"]:\n",
    "                print(sent.text, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f2ecc71-c9a0-4be7-9c44-5a0ad03680a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = open('D://NLP//Frame_NLP//archive//coronavirus_content_sent.txt', 'r', encoding='utf-8').readlines()\n",
    "content_set = set(content)\n",
    "cleandata = open('D://NLP//Frame_NLP//archive//coronavirus_content_sent.txt', 'w', encoding='utf-8')\n",
    "for line in content_set:\n",
    "    cleandata.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d8bbff3-5b86-4a9f-ad19-db5c15d1a6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\miniconda3\\envs\\NLP_env\\lib\\site-packages\\pandas\\util\\_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4328, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent = pd.read_csv(\"D://NLP//Frame_NLP//archive//coronavirus_content_sent.txt\", index_col = False, sep='delimiter', on_bad_lines='skip', header=None, encoding = 'utf8')\n",
    "df_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7455169-08fa-47ea-a1f3-d78a993e143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing for Content\n",
    "List_Content = df_sent[0].to_list()\n",
    "Final_Article = []\n",
    "Complete_Content = []\n",
    "for article in List_Content:\n",
    "    Processed_Content = text_preprocessing(article) #Cleaned text of Content attribute after pre-processing\n",
    "    Final_Article.append(Processed_Content)\n",
    "Complete_Content.extend(Final_Article)\n",
    "df_sent['Processed_Content'] = Complete_Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d22c061-41e4-4d55-9396-2a99fc09946d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4318, 1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent.drop([0], axis=1, inplace=True)\n",
    "df_sent.drop_duplicates(keep = False, inplace = True)\n",
    "df_sent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e11d6db-8341-429b-8c2b-1e3884ed3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent.to_csv('D://NLP//Frame_NLP//archive//coronavirus19_content.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ab7c5-4845-4372-9922-d50a3d6e588d",
   "metadata": {},
   "source": [
    "## Read and Clean Title Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6265a875-aa3a-4e9d-9574-726f1989f8bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Title Dataset \n",
    "df = pd.read_csv(\"D://NLP//Frame_NLP//archive//coronavirus_title.txt\", index_col = False, sep='delimiter', on_bad_lines='skip', header=None, encoding = 'utf8')\n",
    "#DF = pd.read_csv(\"D://NLP//Frame_NLP//archive//covid19_title.txt\", index_col = False, sep='delimiter', encoding = 'utf-8')\n",
    "# Show Dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0587a19d-d77a-4390-80e1-253b7f35e6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Coronavirus Is Airborne. The Coronavirus I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China Coronavirus Outbreak: How Is Coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus In Pennsylvania: Fayette County Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus Outbreak: Busted! Top Myths About ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coronavirus cases in UK confirmed: What are th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Biden says he hasn’t been tested for coronavirus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Germany easing border restrictions as coronavi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Matt Damon's Oldest Daughter Has Recovered Fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Paul Manafort released from prison due to coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>China’s Jilin City in Lockdown to Contain Coro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0\n",
       "0     The Coronavirus Is Airborne. The Coronavirus I...\n",
       "1     China Coronavirus Outbreak: How Is Coronavirus...\n",
       "2     Coronavirus In Pennsylvania: Fayette County Re...\n",
       "3     Coronavirus Outbreak: Busted! Top Myths About ...\n",
       "4     Coronavirus cases in UK confirmed: What are th...\n",
       "...                                                 ...\n",
       "9995   Biden says he hasn’t been tested for coronavirus\n",
       "9996  Germany easing border restrictions as coronavi...\n",
       "9997  Matt Damon's Oldest Daughter Has Recovered Fro...\n",
       "9998  Paul Manafort released from prison due to coro...\n",
       "9999  China’s Jilin City in Lockdown to Contain Coro...\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = df.dropna(axis=0, how='any', inplace=False)\n",
    "df_all\n",
    "df_all.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99a0cb09-d474-46f3-b18c-479d3bab4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing for Title\n",
    "List_Title = df_all[0].to_list()\n",
    "\n",
    "Final_Title = []\n",
    "Complete_Title = []\n",
    "for title in List_Title:\n",
    "    Processed_Title = text_preprocessing(title) #Cleaned text of Title attribute after pre-processing\n",
    "    Final_Title.append(Processed_Title)\n",
    "Complete_Title.extend(Final_Title)\n",
    "df_all['Processed_Title'] = Complete_Title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a3a4a5d0-eea2-47e5-b8b3-1d99fdd5eea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Coronavirus Is Airborne. The Coronavirus Is Airborne. The Coronavirus Is Airborne.',\n",
       " 'China Coronavirus Outbreak: How Is Coronavirus Treated ? ',\n",
       " 'Coronavirus In Pennsylvania: Fayette County Reports First Case Of Coronavirus',\n",
       " 'Coronavirus Outbreak: Busted! Top Myths About Coronavirus',\n",
       " 'Coronavirus cases in UK confirmed: What are the symptoms of coronavirus ? ',\n",
       " 'Is it COVID 19 or covid 19 or coronavirus or coronavirus disease ? ',\n",
       " 'Coronavirus updates: Coronavirus infections in U.S. top 500',\n",
       " 'Coronavirus outbreak: Map tracks Wuhan coronavirus spread',\n",
       " 'Coronavirus Russia deaths: How many coronavirus cases in Russia ? ',\n",
       " \"Coronavirus pandemic China's coronavirus cases drop to one\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Complete_Title[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec3883bb-6942-4077-8044-ba755286fb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Processed_Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Coronavirus Is Airborne. The Coronavirus I...</td>\n",
       "      <td>The Coronavirus Is Airborne. The Coronavirus I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China Coronavirus Outbreak: How Is Coronavirus...</td>\n",
       "      <td>China Coronavirus Outbreak: How Is Coronavirus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus In Pennsylvania: Fayette County Re...</td>\n",
       "      <td>Coronavirus In Pennsylvania: Fayette County Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Coronavirus Outbreak: Busted! Top Myths About ...</td>\n",
       "      <td>Coronavirus Outbreak: Busted! Top Myths About ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Coronavirus cases in UK confirmed: What are th...</td>\n",
       "      <td>Coronavirus cases in UK confirmed: What are th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  The Coronavirus Is Airborne. The Coronavirus I...   \n",
       "1  China Coronavirus Outbreak: How Is Coronavirus...   \n",
       "2  Coronavirus In Pennsylvania: Fayette County Re...   \n",
       "3  Coronavirus Outbreak: Busted! Top Myths About ...   \n",
       "4  Coronavirus cases in UK confirmed: What are th...   \n",
       "\n",
       "                                     Processed_Title  \n",
       "0  The Coronavirus Is Airborne. The Coronavirus I...  \n",
       "1  China Coronavirus Outbreak: How Is Coronavirus...  \n",
       "2  Coronavirus In Pennsylvania: Fayette County Re...  \n",
       "3  Coronavirus Outbreak: Busted! Top Myths About ...  \n",
       "4  Coronavirus cases in UK confirmed: What are th...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29be9d9d-4fb4-48c9-9f51-e3cae2f0db1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.drop([0], axis=1, inplace=True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa9a834b-8d68-41d1-8599-3d17b0199589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8376, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.drop_duplicates(keep = False, inplace = True)\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6304d40f-b46c-491f-a744-edde521cc8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv('D://NLP//Frame_NLP//archive//coronavirus_title.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6074e8a-c541-41ea-9799-2be05caa4198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
